<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="Start here to learn the core concepts of the AIF360 R package. You'll be introduced to the primary components included in the package and how they can be used to detect and mitigate bias.
">
<title>Introduction to AI Fairness 360 • aif360</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.1.0/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.1.0/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Introduction to AI Fairness 360">
<meta property="og:description" content="Start here to learn the core concepts of the AIF360 R package. You'll be introduced to the primary components included in the package and how they can be used to detect and mitigate bias.
">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">aif360</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Released version">0.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/aif360_intro.html">Introduction to AI Fairness 360</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/Trusted-AI/AIF360/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Introduction to AI Fairness 360</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/Trusted-AI/AIF360/blob/HEAD/vignettes/aif360_intro.Rmd" class="external-link"><code>vignettes/aif360_intro.Rmd</code></a></small>
      <div class="d-none name"><code>aif360_intro.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h2>
<p>The AI Fairness 360 (AIF360) package provides a comprehensive set of
tools to detect and mitigate bias in machine learning models. The
package includes:</p>
<ul>
<li>Built-in datasets to understand concepts of fairness,<br>
</li>
<li>Metrics and models for testing biases,</li>
<li>Explanations for these metrics, and</li>
<li>Algorithms to mitigate bias in datasets and models.</li>
</ul>
<p>This vignette introduces the core components of the AIF360 package
and demonstrates how the toolkit can be used to mitigate bias using a
real-world dataset. To get started, you can run the following
commands:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Activate Python environment</span>
<span class="fu">reticulate</span><span class="fu">::</span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/use_python.html" class="external-link">use_condaenv</a></span><span class="op">(</span>condaenv <span class="op">=</span> <span class="st">"r-test"</span>, required <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/Trusted-AI/AIF360" class="external-link">aif360</a></span><span class="op">)</span>
<span class="fu"><a href="../reference/load_aif360_lib.html">load_aif360_lib</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<p>If any issues occur, refer to the installation and troubleshooting
sections of the README in the <a href="https://github.com/Trusted-AI/AIF360/tree/master/aif360/aif360-r" class="external-link">GitHub
Repository</a>.</p>
</div>
<div class="section level2">
<h2 id="data-adult-census-income-dataset">Data: Adult Census Income Dataset<a class="anchor" aria-label="anchor" href="#data-adult-census-income-dataset"></a>
</h2>
<p>The Adult Census Income Dataset is one of four datasets included with
the AIF360 package. Each record consists of census data for an
individual and the goal is to predict whether an individual’s income
exceeds $50k per year. Details on this dataset are documented in
<code><a href="../reference/adult_dataset.html">?adult_dataset</a></code>.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Assumes the Adult data files have been manually added to the correct path</span>
<span class="va">adult</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/adult_dataset.html">adult_dataset</a></span><span class="op">(</span><span class="op">)</span></code></pre></div>
<div class="section level3">
<h3 id="protected-attributes-privileged-groups-and-unprivileged-groups">Protected Attributes, Privileged Groups, and Unprivileged
Groups<a class="anchor" aria-label="anchor" href="#protected-attributes-privileged-groups-and-unprivileged-groups"></a>
</h3>
<p>For a given problem, we must tailor the bias detection and mitigation
toolkit accordingly. This is done by defining a set of attributes
referred to as <em>protected attributes</em> to indicate the particular
bias (or biases) of interest. Additionally, the <em>privileged
groups</em> and <em>unprivileged groups</em> specify the groups that
have historically been at a systematic advantage or disadvantage
respectively.</p>
<p>In our example, the protected attribute will be “sex”, with “1”
(male) and “0” (female) being the values for the privileged and
unprivileged groups respectively.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Protected attribute selection</span>
<span class="va">protected_attribute</span> <span class="op">&lt;-</span> <span class="st">"sex"</span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Privileged and Unprivileged Groups</span>
<span class="co"># The input is a list containing the protected attribute name and the value </span>
<span class="co"># indicating the privileged or unprivileged group respectively</span>

<span class="co"># Privileged Group: Sex is the protected attribute and "1" indicates male </span>
<span class="va">privileged_groups</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">protected_attribute</span>, <span class="fl">1</span><span class="op">)</span> 

<span class="co"># Unprivileged Group: Sex is the protected attribute and "0" indicates female </span>
<span class="va">unprivileged_groups</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">protected_attribute</span>, <span class="fl">0</span><span class="op">)</span> </code></pre></div>
</div>
<div class="section level3">
<h3 id="train-test-split">Train-Test Split<a class="anchor" aria-label="anchor" href="#train-test-split"></a>
</h3>
<p>Before applying bias detection and mitigation techniques, we will
split the original dataset into training and testing datasets. You can
do the train-test split via the <code>split</code> method provided by
the AIF360 dataset object:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1234</span><span class="op">)</span>

<span class="co"># Apply a 70-30 train-test split </span>
<span class="va">adult_split</span> <span class="op">&lt;-</span> <span class="va">adult</span><span class="op">$</span><span class="fu">split</span><span class="op">(</span>num_or_size_splits <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="fl">0.70</span><span class="op">)</span><span class="op">)</span>
<span class="va">adult_train</span> <span class="op">&lt;-</span> <span class="va">adult_split</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span>
<span class="va">adult_test</span>  <span class="op">&lt;-</span> <span class="va">adult_split</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></code></pre></div>
<p>We use only the training dataset in this tutorial, but in typical
machine learning workflows the testing dataset should be used for
assessing the model’s accuracy and fairness.</p>
</div>
</div>
<div class="section level2">
<h2 id="bias-detection">Bias Detection<a class="anchor" aria-label="anchor" href="#bias-detection"></a>
</h2>
<p>We can now use AIF360 to detect bias in the dataset. To do so, the
toolkit provides a wide selection of fairness metrics to choose from
based on whether group fairness, individual fairness, or both group and
individual fairness are desired for the given application. Explore the
available metrics in the documentation via
<code><a href="../reference/binary_label_dataset_metric.html">?binary_label_dataset_metric</a></code>.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Initialize binary label dataset metric class</span>
<span class="va">metric_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/binary_label_dataset_metric.html">binary_label_dataset_metric</a></span><span class="op">(</span><span class="va">adult_train</span>,
                                            privileged_groups <span class="op">=</span> <span class="va">privileged_groups</span>,
                                            unprivileged_groups <span class="op">=</span> <span class="va">unprivileged_groups</span><span class="op">)</span></code></pre></div>
<p>In this application, we are concerned with group fairness. This means
that we want our statistical measure to be equal across the unprivileged
group (females) and privileged group (males). In particular, we will use
the Statistical Parity Difference metric, which is computed as the
difference of the rate of favorable outcomes received by the
unprivileged group to the privileged group. The ideal value of the
Statistical Parity Difference metric is 0.0, with an acceptable range
generally being between -0.1 and 0.1.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Access `Statistical Parity Difference` metric</span>
 <span class="va">metric_train</span><span class="op">$</span><span class="fu">statistical_parity_difference</span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; [1] -0.1977357</span></code></pre></div>
<p>In our example, the metric value is -0.1977357, which is outside the
acceptable range for the Statistical Parity Difference metric. Thus, the
privileged and unprivileged groups are <em>not</em> selected at equal
rates. Meanwhile, the negative value indicates that the unprivileged
group is at the disadvantage.</p>
</div>
<div class="section level2">
<h2 id="bias-mitigation">Bias Mitigation<a class="anchor" aria-label="anchor" href="#bias-mitigation"></a>
</h2>
<p>Bias can be introduced into the system in three main ways:</p>
<ul>
<li>The training dataset may be biased,<br>
</li>
<li>The algorithm that creates the model may be biased, and<br>
</li>
<li>The testing dataset may be biased.</li>
</ul>
<p>The AIF360 toolkit includes <em>pre-processing</em>,
<em>in-processing</em>, and <em>post-processing</em> bias mitigation
techniques to handle each of these three scenarios respectively.</p>
<p>In the previous “Bias Detection” stage of our example, we observed
that the unprivileged group was at a disadvantage in the training
dataset. In this step, we apply the Reweighing algorithm as documented
in <code><a href="../reference/reweighing.html">?reweighing</a></code>. The Reweighing algorithm is a
pre-processing technique that transforms the original dataset to achieve
greater fairness between the privileged and unprivileged groups prior to
the creation of the model.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Pre-processing Algorithm: Reweighing</span>
<span class="va">mitigate_reweighing</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/reweighing.html">reweighing</a></span><span class="op">(</span>privileged_groups <span class="op">=</span> <span class="va">privileged_groups</span>,
                                  unprivileged_groups <span class="op">=</span> <span class="va">unprivileged_groups</span><span class="op">)</span>
<span class="va">adult_transformed</span> <span class="op">&lt;-</span> <span class="va">mitigate_reweighing</span><span class="op">$</span><span class="fu">fit_transform</span><span class="op">(</span><span class="va">adult_train</span><span class="op">)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="effect-of-bias-mitigation-procedure">Effect of Bias Mitigation Procedure<a class="anchor" aria-label="anchor" href="#effect-of-bias-mitigation-procedure"></a>
</h2>
<p>We can now check how effective the Reweighing algorithm from the
previous step was in mitigating bias in the training dataset.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Initialize binary label dataset metric class with the transformed dataset</span>
<span class="va">metric_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/binary_label_dataset_metric.html">binary_label_dataset_metric</a></span><span class="op">(</span><span class="va">adult_transformed</span>,
                                            privileged_groups <span class="op">=</span> <span class="va">privileged_groups</span>,
                                            unprivileged_groups <span class="op">=</span> <span class="va">unprivileged_groups</span><span class="op">)</span>

<span class="co"># Access `Statistical Parity Difference` metric</span>
<span class="va">metric_train</span><span class="op">$</span><span class="fu">statistical_parity_difference</span><span class="op">(</span><span class="op">)</span>
<span class="co">#&gt; [1] -2.775558e-17</span></code></pre></div>
<p>The bias mitigation step was very effective as the metric value is
now -2.775558e-17, very close to 0.0. Thus, we went from a dataset where
the unprivileged group was initially at a disadvantage to one of
equality in terms of the Statistical Parity Difference metric.</p>
</div>
<div class="section level2">
<h2 id="summary">Summary<a class="anchor" aria-label="anchor" href="#summary"></a>
</h2>
<p>This vignette provides an initial introduction to concepts of bias
detection and mitigation in machine learning models using the AI
Fairness 360 toolkit.</p>
<p>As we noted in the vignette, the AI Fairness toolkit provides a wide
array of metrics and bias mitigation algorithms to choose from. Future
vignettes along with the AIF360 website and documentation also discuss
how to use some of these metrics and mitigation algorithms in further
detail.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Gabriela de Queiroz, Stacey Ronaghan, Saishruthi Swaminathan.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.4.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
